# Sanjan B M
/sænˈdʒæn biː ɛm/ • noun • 06:53:00 EET
## About
I'm a student pursuing a Bachelor of Engineering in Computer Science & AI/ML Engineering at ATME College of Engineering in Mysore, with a CGPA of 9.23 so far. I focus on building AI tools that tackle real issues, like automating workflows or analyzing data through machine learning. I see myself as someone who blends hands-on coding with leadership in student groups, keeping my work grounded in practical applications while exploring new tech.

But let's check that self-view. Am I assuming that sticking strictly to AI/ML will always lead to the best outcomes? It might limit exposure to broader fields like cybersecurity or UI/UX design that could make my projects more robust. Another perspective: My mix of internships and projects shows initiative, but could it mean I'm juggling too much, risking depth in any one area? If my logic is that prototypes build skills quickly, that holds up for a student phase, but testing it against finishing more end-to-end deployments might reveal if I'm missing polish for professional roles.

## Experience
### SynerSense Pvt. Ltd.
**VLM [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms) Developer and Research Intern**  
Jun 2025 - Present | Ahmedabad, Gujarat (Remote)  
[https://sanjanb.github.io/Internship-log/](https://sanjanb.github.io/Internship-log/)  
I'm building vision-language pipelines using CLIP and custom VLMs, integrating visual embeddings into MLP classifiers to improve multi-modal accuracy by about 18% on benchmarks. I've also written over 25 pages of technical docs on GitHub Pages to speed up onboarding, and set up MLflow for tracking more than 50 experiments, making comparisons easier. This role sharpens my research skills, but is early specialization assuming VLMs are the future? Maybe, though diversifying into edge AI could add versatility.

### IEEE CS Society – IAMPro
**Deep Learning Project Intern**  
Jun 2024 – Aug 2024 | Bengaluru, Karnataka (Remote)  
I'm developing AI models for pest detection with image recognition, boosting early detection by around 22% over basic CNNs. I used over 20k records of climatic and soil data for crop yield predictions, hitting a 15% accuracy gain, and prototyped generative AI for augmenting limited datasets plus robotics concepts for farming. It ties into sustainable tech, but does assuming data abundance always work? In real fields, sparse data might challenge that—perhaps more focus on transfer learning could help.

### IEEE ATME Student Branch
**Vice Chair**  
Sept 2024 – Present | Mysuru  
I'm leading over five workshops and hackathons on AI/ML, engaging more than 500 students, and coordinating industry talks to bring in fresh insights. This builds on my open-source contributions through SWOC and GirlScript. Am I relying on these for resume boosts? It's valid for networking, but another view: It might dilute time for personal coding. The logic checks if it leads to collaborations, though I'd track how many turn into ongoing projects.

## In Between These Experiences
### The Project Building Journey
My projects trace a path from core ideas to integrated AI systems. It kicked off with the Admission Automation System, a national hackathon winner where I integrated OCR (Tesseract), NLP, and a custom SLM via DistilBERT for data extraction, reaching over 95% accuracy and slashing verification time by 60%. I used FastAPI for modular, scalable pipelines with async handling.
Next, the AI-Powered CTI System, where I co-developed NLP for cyber threat intel using spaCy and Hugging Face Transformers, automating IOC extraction at 90% accuracy, with a FastAPI backend, Jinja2 dashboard, and Docker + Compose for 40% faster deployment.
Then GAN for Fashion Item Generation, developing GANs with TensorFlow and Keras to generate realistic Fashion-MNIST images, boosting quality scores by 25% via LeakyReLU and Batch Normalization, plus MLflow for 30+ experiments.
BERT-Based Sentiment Analysis fine-tuned BERT on 50k IMDB reviews for 94% accuracy, with full preprocessing like tokenization and bias mitigation through augmentation and weighted loss.
Recent GitHub pushes in February 2026 to lifelab (111 commits), follow-through-challenge (44), and my-digital-garden (42) shift toward productivity tools. With 4,526 contributions last year, many private, it's about steady output. But does volume beat targeted depth? Relying on forks means iterating on existing bases—more originals could change that. It adds up if each teaches specifics like GAN stability, but community input might accelerate.
So yes, each one advances the last, even if starters seem basic now.

## Education
### ATME College of Engineering, Mysore
**Bachelor of Engineering in Computer Science & AI/ML Engineering (Hons.)**  
Nov 2022 - Present  
CGPA: 9.23  
Core subjects include Deep Learning, Python for Data Science, Data Structures, DBMS, OS, Probability, Discrete Structures. Am I assuming high grades guarantee opportunities? They help, but real-world impact from projects might matter more in hiring.

## Research Publications
I haven't listed full publications yet, but contributed to AI/ML research with papers submitted under ATME, focusing on deep learning and NLP. As a "Published Researcher" award notes, it's a start—submitting to journals could formalize it.

## Tech Stack
I'm adaptable, but these stand out:  
Languages: Python (Advanced), C (Intermediate), SQL (Intermediate), Java, TypeScript  
Libraries/Frameworks: PyTorch, TensorFlow, Keras, Scikit-Learn, Hugging Face Transformers, MLflow, FastAPI, Flask, NumPy, Pandas  
Tools: Git, GitHub, Jupyter, VS Code, Google Colab, Docker, MongoDB  
Specializations: LLMs (BERT, GPT, CLIP, VLMs), Generative AI, MLOps, NLP, Deep Learning, Computer Vision. Generalist for now, but could depth in VLMs assume that's the hot area? Balancing with cloud skills might broaden appeal.

## Recommendations by Clients
Not publicly detailed yet, but my hackathon wins and IEEE role imply strong feedback. Collecting endorsements on LinkedIn would strengthen this.

## Explainer Videos
No dedicated channel, but demos paired with my blog posts could explain projects like the CTI system—YouTube would make them more accessible.

## Writings & Blogs
I share thoughts on my portfolio site, keeping it simple.  
- Sep 13, 2025: My AI/ML Specialization Journey: From Foundations to Advanced LLM Applications  
- Jan 15, 2025: Advanced Blog Formatting Guide: Mastering Jekyll and al-folio Techniques  
[https://sanjanb.github.io/](https://sanjanb.github.io/)  
Is a personal site assuming enough reach? Medium could expand audience, but control here is key. Adding project deep-dives, like bias in BERT, would add value.

## Library
Not formally listed, but based on my work, reads like "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" or "Weapons of Math Destruction" for ethics could fit well.

## Thing About Me
Outside tech, I'm a Kaggle Practitioner since May 2024, competing in ML/DL challenges for preprocessing and ensemble skills. I also volunteer in community efforts, drawing from awards like Best Student for extracurriculars. It keeps me balanced, but is it automatic? Scheduling non-tech time prevents burnout.

## Get in Touch
Reach me on [LinkedIn](https://www.linkedin.com/in/sanjan-bm/) or email at sanjanaacharaya1234@gmail.com (subject: Project Inquiry). Phone: +91 9535752673
---
**Links:**
- GitHub: [https://github.com/sanjanb](https://github.com/sanjanb)
- LinkedIn: [https://www.linkedin.com/in/sanjan-bm/](https://www.linkedin.com/in/sanjan-bm/)
- Portfolio: [https://sanjanb.github.io/](https://sanjanb.github.io/)
- Kaggle: [https://www.kaggle.com/sanjanbm](https://www.kaggle.com/sanjanbm)